---
layout: page
title: Hi, I'm Yanwu Xu
subtitle: PhD student in Artificial Intellegence
use-site-title: true
---

<h3>Short Bio</h3>
<p>My name is Yanwu Xu, currently a 3nd-year PhD student in Intellegent System at 
  University of Pittsburgh, advised by <a href="https://kayhan.dbmi.pitt.edu">Dr. Kayhan Batmanghelich</a> 
  and co-advised by <a href="https://mingming-gong.github.io/">Dr. Mingming Gong</a>.
  My current research interests are <b>Generative Adversarial Learning </b> and <b>Domain Transfer</b>. 
  Before that, I obtained B.S. degree in Electrical Mechanical Engineering department at Central South University in 2017.</p>


<h3>Blog</h3>
<ul>
  <li><p>Congrats to our Image2Image translation model accpeted by CVPR2022. In this work, we proposed an adversarial spatial perturbation consistency
    for I2I task. More detials, please refer to our <a href="https://github.com/batmanlab/MSPC">git repo</a>. <br/>
    
    Unpaired image-to-image translation (I2I) is an ill- posed problem, as an infinite number of translation func- tions can map the source domain 
    distribution to the tar- get distribution. Therefore, much effort has been put into designing suitable constraints, e.g., cycle consistency 
    (Cy- cleGAN), geometry consistency (GCGAN), and contrastive learning-based constraints (CUTGAN), that help better pose the problem. However, 
    these well-known constraints have limitations: (1) they are either too restrictive or too weak for specific I2I tasks; (2) these methods result 
    in con- tent distortion when there is a significant spatial variation between the source and target domains. This paper pro- poses a universal 
    regularization technique called maximum spatial perturbation consistency (MSPC), which enforces a spatial perturbation function (T ) and the 
    translation oper- ator (G) to be commutative (i.e., T ◦ G = G ◦ T ). In ad- dition, we introduce two adversarial training components for learning 
    the spatial perturbation function. The first one lets T compete with G to achieve maximum perturbation. The second one lets G and T compete with 
    discriminators to align the spatial variations caused by the change of ob- ject size, object distortion, background interruptions, etc. Our 
    method outperforms the state-of-the-art methods on most I2I benchmarks. We also introduce a new benchmark, namely the front face to profile 
    face dataset, to emphasize the underlying challenges of I2I for real-world applications. We finally perform ablation experiments to study the 
    sensi- tivity of our method to the severity of spatial perturbation and its effectiveness for distribution alignment.
</ul>


<h3>News</h3>
<ul>
  <li><p>One papers is accepted by <b>CVPR 2022</b>.</p></li>
  <li><p>One papers is accepted by <b>ACM 2021</b>.</p></li>
  <li><p>One papers is accepted by <b>ICCV 2021</b>.</p></li>
  <li><p>Two papers are accepted by <b>AAAI 2020</b>.</p></li>
  <li><p>One paper is accepted by <b>NeurIPS 2019</b>.</p></li>
  <li><p>One paper is accepted by <b>BraTS Challenge 2018</b>.</p></li>
  <li><p>One paper is accepted by <b>ACCV 2018</b>.</p></li>
</ul>

